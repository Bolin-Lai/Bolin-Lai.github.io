<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Bolin Lai</title>

    <meta name="author" content="Bolin Lai">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:60%;vertical-align:middle">
                <p class="name" style="text-align: center;">Bolin Lai</p>
                <p>Hi! I am a third-year PhD student in the Machine Learning Program of Georgia Institute of Technology, advised by <a href="https://rehg.org/">Prof. James Rehg</a> and co-advised by <a href="https://faculty.cc.gatech.edu/~zk15/">Prof. Zsolt Kira</a>. Prior to starting my PhD, I got my Master's degree majoring in ECE and Bachelor's degree majoring in Information Engineering from Shanghai Jiao Tong University. I worked with <a href="https://scholar.google.com/citations?user=pbjw9sMAAAAJ&hl=en&oi=ao">Prof. Ya Zhang</a> during my master.</p>
                <p style="text-align:center">
                  <a href="mailto:bolin.lai@gatech.edu">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=lWrljmQAAAAJ&hl=en"> Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/BolinLai">Github</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/bolin-lai-625b78139">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/bryanislucky">Twitter</a>
                </p>
                <!-- <p><font color="#FF8080">I'm actively looking for a research internship opportunity in summer 2024.</font></p> -->
                <p><font color="#FF8080">I'm looking for self-motivated graduate/ungraduate students to collaborate. Don't hesitate to reach out to me if you are interested.</font></p>
              </td>
              <td style="padding:2.5%;width:35%;max-width:40%">
                <img style="width:100%;max-width:100%;object-fit:cover;" alt="profile photo" src="figures/photo.png">
                <!-- <img style="width:100%;max-width:100%;object-fit:cover;border-radius: 50%;" alt="profile photo" src="figures/photo.png"> -->
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:10px;width:100%;vertical-align:middle">
                <h2>Research Interests</h2>
              </td>
            </tr>
            <tr>
              <td style="padding:10px;width:100%;vertical-align:middle">
                <p>
                  My research interests lie in <b>Multi-Modal Learning</b>, <b>Generative Models</b> and <b>Video Understanding</b>. 
                  <!-- Currently, I'm focusing on joint representation learning of vision and language in videos as well as image/video generation using diffusion models. -->
                  Currently, I'm focusing on advancing multi-modal unerstanding and generation through the integration of Large Language Models (LLMs) and Diffusion Models (DMs), aiming to connect and leverage the latent representation spaces of these two model architectures.
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:10px;width:100%;vertical-align:middle">
                <h2>News</h2>
              </td>
            </tr>
            <tr>
              <td style="padding:5px;width:100%;vertical-align:middle">
                <ul>
                  <li>Aug 2024: Our LEGO paper got Oral presentation.</li>
                  <li><font color="#fb8b23"><strong>July 2024: Two first-author papers were accepted by ECCV! Please check out our latest work: <a href="https://bolinlai.github.io/Lego_EgoActGen/">LEGO (for action generation)</a> and <a href="https://bolinlai.github.io/CSTS-EgoGazeAnticipation/">CSTS (for gaze forecasting)</a>. Thank all the co-authors!</strong></font></li>
                  <li>May 2024: I started my second intenrship at GenAI, Meta in Bay Area.</li>
                  <li>Mar 2024: One co-author paper was accepted by CVPR (Oral). See you in Seattle!</li>
                  <li>Jul 2023: Our expansion of prior work GLC was accepted by IJCV!</li>
                  <li>May 2023: I started my internship at GenAI Meta in Bay Area!</li>
                  <li>Apr 2023: I successfully passed the qualifying exam.</li>
                  <li>Mar 2023: One paper was accepted to the Findings of ACL2023. Please check out our new dataset for social understanding: <a href="https://aclanthology.org/2023.findings-acl.411.pdf">Werewolf Among Us</a>.</li>
                  <li>Nov 2022: We won the Best Student Paper Prize on BMVC. Thanks to all co-authors!</li>
                  <li>Sep 2022: Our work <a href="https://bolinlai.github.io/GLC-EgoGazeEst/">GLC</a> was accepted by <a href="https://bmvc2022.org/">BMVC 2022</a>!</li>
                  <li>Jan 2022: I started working with <a href="https://rehg.org/">Prof. James Rehg</a> at Georgia Tech.</li>
                </ul>
              </td>
            </tr>
          </tbody></table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:10px;width:100%;vertical-align:middle">
                <h2>Publications</h2>
              </td>
            </tr>
          </tbody></table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:40%;vertical-align:middle;">
                <div class="one">
                  <img src='figures/lego.png' width="100%">
                </div>
              </td>
              <td style="padding:20px;width:60%;vertical-align:middle;">
                <span class="papertitle">LEGO: <u>L</u>earning <u>EGO</u>centric Action Frame Generation via Visual Instruction Tuning</span>
                <br>
                <br>
                <strong>Bolin Lai</strong>,
                <a href="https://sites.google.com/view/xiaoliangdai/">Xiaoliang Dai</a>,
                <a href="https://www.lawrencechen.me/">Lawrence Chen</a>,
                <a href="https://scholar.google.com/citations?user=7v1LZxUAAAAJ&hl=en">Guan Pang,</a>
                <a href="https://rehg.org/">James M. Rehg</a>,
                <a href="https://aptx4869lm.github.io/">Miao Liu</a>
                <br>
                <font color='#A51014'><em>ECCV</em>, 2024 (Oral)</font>
                <br>
                <a href="https://bolinlai.github.io/Lego_EgoActGen/">Webpage</a> /
                <a href="https://arxiv.org/pdf/2312.03849.pdf">Paper</a> /
                <a href="https://github.com/BolinLai/LEGO">Code</a> /
                <a href="https://www.dropbox.com/scl/fo/4m0v9oy753aimas8rz6v1/ANoJhZQz2BdcGIVLzUsHdP0?rlkey=o8saklcszfc098mjnpid767ic&dl=0">Dataset</a> / Supplementary
              </td>
            </tr>
            
            <tr>
              <td style="padding:20px;width:40%;vertical-align:middle;">
                <div class="one">
                  <img src='figures/avgaze.png' width="90%">
                </div>
              </td>
              <td style="padding:20px;width:60%;vertical-align:middle;">
                <span class="papertitle">Listen to Look into the Future: Audio-Visual Egocentric Gaze Anticipation</span>
                <br>
                <br>
                <strong>Bolin Lai</strong>,
                <a href="https://fkryan.github.io/">Fiona Ryan</a>,
                <a href="https://vjwq.github.io/">Wenqi Jia</a>,
                <a href="https://aptx4869lm.github.io/">Miao Liu*</a>,
                <a href="https://rehg.org/">James M. Rehg*</a>
                <br>
                <font color='#A51014'><em>ECCV</em>, 2024</font>
                <br>
                <a href="https://bolinlai.github.io/CSTS-EgoGazeAnticipation/">Webpage</a> /
                <a href="https://arxiv.org/pdf/2305.03907.pdf">Paper</a> /
                <a href="https://github.com/BolinLai/CSTS">Code</a> / <a href="https://github.com/BolinLai/CSTS/tree/main/data">Data Split</a> / Supplementary
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:40%;vertical-align:middle">
                <div class="one">
                  <img src='figures/multimodal_social.png' width="100%">
                </div>
              </td>
              <td style="padding:20px;width:60%;vertical-align:middle">
                <span class="papertitle">Modeling Multimodal Social Interactions: New Challenges and Baselines
                  with Densely Aligned Representations</span>
                <br>
                <br>
                <a href="https://sites.google.com/view/sangmin-lee">Sangmin Lee</a>,
                <strong>Bolin Lai</strong>,
                <a href="https://fkryan.github.io/">Fiona Ryan</a>,
                <a href="https://scholar.google.com/citations?user=e4jgsUcAAAAJ&hl=en">Bikram Boote</a>,
                <a href="https://rehg.org/">James M. Rehg</a>,
                <br>
                <font color='#A51014'><em>CVPR</em>, 2024 (Oral) [Acceptance Rate 0.8%]</font>
                <br>
                <a href="https://sangmin-git.github.io/projects/MMSI/">Webpage</a> /
                <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Lee_Modeling_Multimodal_Social_Interactions_New_Challenges_and_Baselines_with_Densely_CVPR_2024_paper.pdf">Paper</a> /
                <a href="https://github.com/sangmin-git/MMSI">Code</a> /
                <a href="https://www.dropbox.com/scl/fo/fbv6njzu1ynbgv9wgtrwo/ANPk2TKqK2rl44MqKu05ogk?rlkey=yx7bmzmmiymauvz99q2rvjajg&st=305631zj&dl=0">Split & Annotations</a> /
                <a href="https://openaccess.thecvf.com/content/CVPR2024/supplemental/Lee_Modeling_Multimodal_Social_CVPR_2024_supplemental.pdf">Supplementary</a>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:40%;vertical-align:middle">
                <div class="one">
                  <img src='figures/deduction.png' width="100%">
                </div>
              </td>
              <td style="padding:20px;width:60%;vertical-align:middle">
                <span class="papertitle">Werewolf Among Us: Multimodal Resources for Modeling Persuasion Behaviors in Social Deduction Games</span>
                <br>
                <br>
                <strong>Bolin Lai</strong>*,
                <a href="https://icefoxzhx.github.io/">Hongxin Zhang*</a>,
                <a href="https://aptx4869lm.github.io/">Miao Liu*</a>,
                Aryan Pariani*,
                <a href="https://fkryan.github.io/">Fiona Ryan</a>,
                <a href="https://vjwq.github.io/">Wenqi Jia</a>,
                <a href="https://www.shirley.id/">Shirley Anugrah Hayati</a>,
                <a href="https://rehg.org/">James M. Rehg</a>,
                <a href="https://cs.stanford.edu/~diyiy/">Diyi Yang</a>
                <br>
                <font color='#A51014'><em>ACL Findings</em>, 2023</font>
                <br>
                <a href="https://persuasion-deductiongame.socialai-data.org/">Webpage</a> /
                <a href="https://aclanthology.org/2023.findings-acl.411.pdf">Paper</a> /
                <a href="https://github.com/SALT-NLP/PersuationGames">Code</a> /
                <a href="https://drive.google.com/drive/folders/1N4PymMbKXFzqy3fq4ZGjdrc0oiScV914">Dataset</a> /
                <a href="https://drive.google.com/file/d/10siMUmwy8coWeX39p3AfbGhzlt496GSO/view?usp=drive_link">Video</a>
                <!-- <p>Modeling social behaviors of multi-person scenarios using both video and language.</p> -->
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:40%;vertical-align:middle">
                <div class="one">
                  <img src='figures/gaze_est.png' width="100%">
                </div>
              </td>
              <td style="padding:20px;width:60%;vertical-align:middle">
                <span class="papertitle">In the Eye of Transformer: Global-Local Correlation for Egocentric Gaze Estimation and Beyond</span>
                <br>
                <br>
                <strong>Bolin Lai</strong>,
                <a href="https://aptx4869lm.github.io/">Miao Liu</a>,
                <a href="https://fkryan.github.io/">Fiona Ryan</a>,
                <a href="https://rehg.org/">James M. Rehg</a>
                <br>
                <font color='#A51014'><em>International Journal of Computer Vision (IJCV)</em>, 2023</font>
                <br>
                <a href="https://bolinlai.github.io/GLC-EgoGazeEst/">Webpage</a> /
                <a href="https://link.springer.com/article/10.1007/s11263-023-01879-7">Paper</a> /
                <a href="https://github.com/BolinLai/GLC">Code</a>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:40%;vertical-align:middle">
                <div class="one">
                  <img src='figures/glc.png' width="100%">
                </div>
              </td>
              <td style="padding:20px;width:60%;vertical-align:middle">
                <span class="papertitle">In the Eye of Transformer: Global-Local Correlation for Egocentric Gaze Estimation</span>
                <br>
                <br>
                <strong>Bolin Lai</strong>,
                <a href="https://aptx4869lm.github.io/">Miao Liu</a>,
                <a href="https://fkryan.github.io/">Fiona Ryan</a>,
                <a href="https://rehg.org/">James M. Rehg</a>
                <br>
                <font color='#A51014'><em>BMVC</em>, 2022 (Best Student Paper)</font>
                <br>
                <a href="https://bolinlai.github.io/GLC-EgoGazeEst/">Webpage</a> /
                <a href="https://bmvc2022.mpi-inf.mpg.de/0227.pdf">Paper</a> /
                <a href="https://github.com/BolinLai/GLC">Code</a> /
                <a href="https://bolinlai.github.io/GLC-EgoGazeEst/Ego4D_Gaze_Split.zip">Data Split</a> /
                <a href="https://bmvc2022.mpi-inf.mpg.de/0227_supp.zip">Supplementary</a> /
                <a href="https://bmvc2022.mpi-inf.mpg.de/0227_video.mp4">Video</a> /
                <a href="https://bmvc2022.mpi-inf.mpg.de/0227_poster.pdf">Poster</a>
                <!-- <p>Improving egocentric gaze estimation by explicitly modeling global-local correlations in transformer-based architecture.</p> -->
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  ---------------- Research before my PhD, mainly about medical image analysis ----------------
                </td>
              </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:40%;vertical-align:middle">
                <div class="one">
                  <img src='figures/venibot.png' width="100%">
                </div>
              </td>
              <td style="padding:20px;width:60%;vertical-align:middle">
                <span class="papertitle">Semi-supervised Vein Segmentation of Ultrasound Images for Autonomous Venipuncture</span>
                <br>
                <br>
                Yu Chen, Yuxuan Wang, <strong>Bolin Lai</strong>, Zijie Chen, Xu Cao,
                <a href="https://ynysjtu.github.io/">Nanyang Ye</a>,
                Zhongyuan Ren, Junbo Zhao, 
                <a href="https://xiaoyunzhou27.github.io/xiaoyunzhou/">Xiao-Yun Zhou</a>,
                <a href="https://nms.kcl.ac.uk/core/?page_id=44">Peng Qi</a>
                <br>
                <font color='#A51014'><em>IROS</em>, 2021</font>
                <br>
                [<a href="https://arxiv.org/pdf/2105.12945">Paper</a>]
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:40%;vertical-align:middle">
                <div class="one">
                  <img src='figures/semi_tumor.png' width="100%">
                </div>
              </td>
              <td style="padding:20px;width:60%;vertical-align:middle">
                <span class="papertitle">Hetero-Modal Learning and Expansive Consistency Constraints for Semi-Supervised Detection from Multi-Sequence Data</span>
                <br>
                <br>
                <strong>Bolin Lai</strong>, Yuhsuan Wu, 
                <a href="https://xiaoyunzhou27.github.io/xiaoyunzhou/">Xiao-Yun Zhou</a>,
                Peng Wang,
                <a href="https://lelu007.github.io/">Le Lu</a>,
                Lingyun Huang,
                Mei Han, Jing Xiao, Heping Hu, 
                <a href="https://extragoya.github.io/">Adam P. Harrison</a>
                <br>
                <font color='#A51014'><em>Machine Learning in Medical Imaging</em>, 2021</font>
                <br>
                [<a href="https://arxiv.org/pdf/2103.12972.pdf">Paper</a>]
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:40%;vertical-align:middle">
                <div class="one">
                  <img src='figures/ksp.png' width="100%">
                </div>
              </td>
              <td style="padding:20px;width:60%;vertical-align:middle">
                <span class="papertitle">Liver Tumor Localization and Characterization from Multi-phase MR Volumes Using Key-Slice Prediction: A Physician-Inspired Approach</span>
                <br>
                <br>
                <strong>Bolin Lai</strong>*, Yuhsuan Wu*, Xiaoyu Bai*,
                <a href="https://xiaoyunzhou27.github.io/xiaoyunzhou/">Xiao-Yun Zhou</a>,
                Peng Wang,
                <a href="https://jimmycai91.github.io/">Jinzheng Cai</a>,
                <a href="https://hrlblab.github.io/">Yuankai Huo</a>,
                Lingyun Huang,
                <a href="https://jszy.nwpu.edu.cn/en/yongxia.html">Yong Xia</a>,
                Jing Xiao,
                <a href="https://lelu007.github.io/">Le Lu</a>,
                Heping Hu,
                <a href="https://extragoya.github.io/">Adam P. Harrison</a>
                <br>
                <font color='#A51014'><em>International Workshop on PRedictive Intelligence In MEdicine</em>, 2021</font>
                <br>
                [<a href="https://www.researchgate.net/profile/Adam-Harrison-6/publication/354887409_Liver_Tumor_Localization_and_Characterization_from_Multi-phase_MR_Volumes_Using_Key-Slice_Prediction_A_Physician-Inspired_Approach/links/61def5ba4e4aff4a643863ea/Liver-Tumor-Localization-and-Characterization-from-Multi-phase-MR-Volumes-Using-Key-Slice-Prediction-A-Physician-Inspired-Approach.pdf">Paper</a>]
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:40%;vertical-align:middle">
                <div class="one">
                  <img src='figures/spinal_dislocation.png' width="100%">
                </div>
              </td>
              <td style="padding:20px;width:60%;vertical-align:middle">
                <span class="papertitle">Spatial Regularized Classification Network for Spinal Dislocation Diagnosis</span>
                <br>
                <br>
                <strong>Bolin Lai</strong>, Shiqi Peng, Guangyu Yao,
                <a href="https://scholar.google.com/citations?user=pbjw9sMAAAAJ&hl=en&oi=ao">Ya Zhang</a>,
                Xiaoyun Zhang, Yanfeng Wang, Hui Zhao
                <br>
                <font color='#A51014'><em>Machine Learning in Medical Imaging</em>, 2019</font>
                <br>
                [<a href="https://www.researchgate.net/profile/Adam-Harrison-6/publication/354887409_Liver_Tumor_Localization_and_Characterization_from_Multi-phase_MR_Volumes_Using_Key-Slice_Prediction_A_Physician-Inspired_Approach/links/61def5ba4e4aff4a643863ea/Liver-Tumor-Localization-and-Characterization-from-Multi-phase-MR-Volumes-Using-Key-Slice-Prediction-A-Physician-Inspired-Approach.pdf">Paper</a>]
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Service</h2>
              </td>
            </tr>
          </tbody></table>

          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <tr>
              <td width="100%" valign="center">
                Reviewer for <br>
                - Computer Vision and Pattern Recognition Conference (CVPR) <br>
                - European Conference on Computer Vision (ECCV) <br>
                - The Association for Computational Linguistics (ACL) <br>
                - Empirical Methods in Natural Language Processing (EMNLP) <br>
                - International Journal of Computer Vision (IJCV) <br>
                - Association for the Advancement of Artificial Intelligence (AAAI) <br>
                - International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI) <br>
                - Journal of Biomedical and Health Informatics (JBHI) <br>
                - IEEE Signal Processing Letters (SPL)
                <br>
                <br>
                Taught ECE4871 as a teacher assistant at Georgia Tech in 2021 and 2022.
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  This website is adapted from this <a href="https://github.com/jonbarron/jonbarron_website">source code</a>.
                </p>
              </td>
            </tr>
          </tbody></table>

        </td>
      </tr>
    </table>
  </body>
</html>
